\documentclass[12pt]{article}
\usepackage{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{appendix}
\usepackage{multicol}
\usepackage{stmaryrd}
\usepackage{algorithm, algorithmic}

\theoremstyle{plain}
\newtheorem{thm}{Théorème}[section]
\newtheorem{lemme}[thm]{Lemme}
\newtheorem{propo}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{remarque}[thm]{Remarque}
\newtheorem{defi}[thm]{Définition}
\newtheorem{nota}[thm]{Notation}


\renewcommand{\a}{\alpha}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
\newcommand{\F}{\mathbb{F}} 
\newcommand{\A}{\mathcal{A}} 
\newcommand{\e}{\mathbf{e}}  
\newcommand{\s}{\mathbf{s}} 
\newcommand{\K}{\mathbb{K}} 
\newcommand{\J}{\mathcal{J}} 
\newcommand{\D}{\mathcal{D}} 

% traduction des mots pour les algos
\floatname{algorithm}{Algorithme}
\renewcommand{\algorithmicrequire}{\textbf{Entrées:}}
\renewcommand{\algorithmicensure}{\textbf{Sortie:}}
\renewcommand{\algorithmicrepeat}{\textbf{Faire}}
\renewcommand{\algorithmicuntil}{\textbf{Tant que}}
\renewcommand{\algorithmicreturn}{\textbf{Retourne}}

\renewcommand{\contentsname}{Sommaire}

\title{Projet - Wave}
\author{Lansade Suzanne}
\author{Palandjian Eva}

\begin{document}

\begin{titlepage}

\center

\textsc{\LARGE Université de Bordeaux}\\[2.0cm]
\textsc{\Large Master 2 : Cryptologie et Sécurité Informatique}\\[0.5cm] 
\textsc{\large Projet de fin d'études}\\[1.2cm] 

\HRule \\[0.4cm]
{ \huge \bfseries Wave - Un procédé de signature \\ à base de codes correcteurs}\\[0.3cm] 
\HRule \\[1.3cm]


\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Suzanne \textsc{Lansade}\\
Eva \textsc{Palandjian}\\
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Encadrant:} \\
Gilles \textsc{Zemor}
\end{flushright}
\end{minipage}\\[2cm]

\begin{large}
Février, 2020
\end{large}

\vspace{0.5in}

\includegraphics [scale=0.2]{include/Universite_Bordeaux.jpg}

\end{titlepage}

\newpage
\tableofcontents
\newpage

\section*{Introduction}
\addcontentsline{toc}{section}{\protect\numberline{}Introduction}

- Passage au post-quantique --> On cherche des alternatives à RSA et DH.\\
- Appel d'offre NIST \\
- On aimerai bien un système de signature utilisant les codes, puisque le problème du décodage d'un code aléatoire est NP-complet. \\
- Ça marche pas mal pour le chiffrement, mais encore très compliqué pour les signatures, comme en témoigne le tableau des soumissions au NIST pour le second tour. \\

\begin{figure}[h]
\label{nist 2}
\begin{center}
\includegraphics [scale=0.4]{include/nist_second_tour.png}
\end{center}
\caption{\small Comparaison des soumissions au NIST du second tour.}
\end{figure}
\noindent Alors pourquoi est-il si compliqué de faire des signatures avec des codes correcteurs ? \\
- car il est difficile de tomber dans l'ensemble des syndromes facilement décodables, ie. difficile de créer une fonction de hachage qui envoie le message $m$ dans l'ensemble des syndromes possibles. En effet, pour un décodage au sens stricte il faudrait un syndrome $s$ associé à un unique mot $c$ du code, le plus proche de $m$.\\
quand on chiffre -> ne pose pas de problème. \\
quand on signe -> pose un problème car il est dur de trouver un syndrome de cette sorte. \\
Les solutions à ce problème ont pour l'instant soit été cassées, soit impraticables.\\
La solution de Wave est d'enlever la restriction au mot le plus proche. On cherche maintenant une famille de codes permettant de trouver, pour un mot quelconque, un des mots de code à distance $\omega$. En particulier, la distance de décodage $\omega$ est très grande, ce qui assure typiquement de l’existence d’un mot de code à distance $\omega$.\\
Explication rapide du schéma wave + on cherchera à rendre les sorties uniformes + preuves de sécurité. \\

\section{Le schéma de signature Wave}
Nous allons détailler dans cette section le schéma de signature Wave. C'est un schéma de type hache et signe à base de codes correcteurs. Pour des raisons de clarté nous oublierons dans un premier temps la problématique du hachage. Nous la réintroduirons en fin de rapport afin de proposer une preuve formelle de la sécurité du schéma, où la fonction de hachage est alors nécessaire. Pour l'instant, nous considérerons que l'entrée de l'algorithme de signature est déjà un syndrome du code considéré.\\
Le schéma de signature Wave s'appuie sur une famille de codes appelés des codes $(U,U+V)$-généralisés. La structure de ces codes nous permettra de proposer un algorithme de décodage $\mathcal{D}$ utilisant une trappe $T$ et donnant un avantage par rapport à un algorithme de décodage générique. Ce système s'appuie aussi sur la notion de fonctions GPV en moyenne, que nous détaillerons.

\subsection{La famille de codes (U,U+V)-généralisés}
Pour définir la famille de code utilisée dans le schéma de signature Wave, on part de deux codes de longueur $n/2$ aléatoires de dimensions respectives $k_u$ et $k_v$. On a premièrement la définition suivante :

\begin{defi}\label{UV} (Code (U,U+V).) Un code $(U,U+V)$ est un code de longueur $n$ et de dimension $k=k_U+k_V$ et tel que :
\begin{center}
$(U,U+V) = \{(u,u+v)$ tel que $u \in U$ et $v \in V \}$
\end{center}
\end{defi}


\noindent En revanche, pour le système de signature Wave, nous n'utiliserons pas cette définition. En effet, on peut facilement tirer des informations sur la structure de ces codes.\\


\begin{defi}($hull$).
Le  $hull$ d'un code $\mathcal{C}$ est défini comme $hull(\mathcal{C}) := \mathcal{C} \cap \mathcal{C}^{\bot}$
\end{defi}

\begin{propo}\label{dim_hull}
Soit $\mathcal{C}$ un code aléatoire binaire de longueur $n$ et de dimension $k$, alors on s'attend à avoir $dim(hull(\mathcal{C}))\sim\mathcal{O}(1)$. De plus : $$ \mathbb{P}\big(dim(hull(\mathcal{C})\big) \leq t) \ \geq\ 1 - \mathcal{O}(2^{-t}) $$
\end{propo}

\begin{propo}\label{dim_hull_UV}
Soit $UV$ un code $(U,U+V)$ binaire permuté de longueur $n$ tel que $k_U > k_V$, alors nous avons avec probabilité $1-\mathcal{O}(2^{k_U-k_V})$
$$ dim(hull(UV)) = k_U - k_V $$
\end{propo}

\begin{proof}[Preuve]
En annexe.
\end{proof}

\noindent Ainsi nous ne pouvons pas utiliser les codes $(U,U+V)$ binaires où $\dim(U) > \dim(V)$ puisque nous pouvons facilement les distinguer de codes aléatoires.
En effet, il suffit de calculer la dimension de  leur $hull$ puis de le comparer au résultat de la proposition \ref{dim_hull}.
Pour résoudre ce problème, nous poserons $q=3$ pour toute la suite du rapport.
Nous utiliserons également une nouvelle famille de codes, les codes $(U,U+V)$-généralisés :\\
\begin{defi} \label{UV-normalise} (codes $(U,U+V)$-généralisés) Soient $n$ un entier pair et $a$,$b$,$c$,$d$ quatres vecteurs de $\F_q^{n/2}$ tels que pour tout $i \in \llbracket 1,n/2\rrbracket$ :
\begin{equation}\label{ac}
a_ic_i \neq 0 
\end{equation}
\begin{equation}\label{ad-bc}
a_id_i - b_ic_i \neq 0 
\end{equation}

\noindent Soient U et V deux codes définis comme précédemment. Le code $(U,U+V)$-généralisé correspond à l'ensemble :
\begin{center}
$\{(a.u + b.v, c.u + d.v)$ tel que $u \in U$ et $v \in V \}$
\end{center}
où $x.y$ est le produit coordonnée par coordonnée des $x_i$ et $y_i$.\\
\end{defi}


\noindent Les conditions sur les vecteurs $a$, $b$, $c$, $d$ permet de garantir que :
\begin{itemize}
\item[-] toutes les coordonnées de $u \in U$ apparaîtront deux fois, ce qui sera nécessaire pour utiliser la structure du code dans notre algorithme de décodage (par l'équation \eqref{ac}).
\item[-] la dimension du code $(U,U+V)$-généralisé sera la somme des dimensions des codes $U$ et $V$ (par l'équation \eqref{ad-bc}).
\end{itemize}

\noindent Sans perte de généralité, nous posons pour toute la suite les vecteurs $a$,$b$,$c$,$d$ tels que $a_id_i - b_ic_i = 1 \text{ pour tout } i \in \llbracket 1,n/2\rrbracket$. \\

\begin{propo} Soient $U$, $V$, $a$, $b$, $c$ et $d$ définis comme précédemment. Soit $UV$ le code $(U,U+V)$-généralisé associé. Alors
$$ k = \dim\; (UV) = k_U + k_V.$$
De plus soient $G_U \in \F_q^{k_U \times n/2}$ (respectivement $G_V \in \F_q^{k_V \times n/2}$) et $H_U \in \F_q^{(n/2-k_U) \times n/2}$ (respectivement $H_V \in \F_q^{(n/2-k_V) \times n/2}$) les matrices génératrices et de parité des codes $U$ et $V$. Soient $A$, $B$, $C$, $D$ de $\F_q^{n \times n}$ les matrices diagonales de diagonale respectives les vecteurs $a$, $b$, $c$ et $d$.  \\
\vspace{0.2in}
Alors la matrice de $\F_q^{(k_U + k_V) \times n}$: 

\vspace{0.1in}

$$
G := 
\begin{pmatrix}
\begin{array}{c|c}
G_UA & G_UC \\
 \hline 
G_VB & G_VD \\
\end{array} \\
\end{pmatrix}
$$

\noindent et la matrice $\F_q^{(n - k_U - k_V) \times n}$:

\vspace{0.1in}
$$ 
H :=
\begin{pmatrix}
\begin{array}{c|c}
H_UD & -H_UB \\
 \hline 
-H_VC & H_VA \\
\end{array} \\
\end{pmatrix}
$$
\vspace{0.1in}

\noindent sont des matrices génératrices et de parité du code $UV$. 
\end{propo}

\begin{proof}[Preuve]
Remarquons d'abord que G engendre bien le code $UV$. Remarquons aussi que 

$$
G = 
\begin{pmatrix}
\begin{array}{c|c}
G_UA & G_UC \\
 \hline 
G_VB & G_VD \\
\end{array} \\
\end{pmatrix}
= 
\begin{pmatrix}
\begin{array}{c|c}
G_U & 0 \\
 \hline 
0 & G_V \\
\end{array} \\
\end{pmatrix} 
\begin{pmatrix}
\begin{array}{c|c}
A & C \\
 \hline 
B & D \\
\end{array} \\
\end{pmatrix}
$$

\noindent Par définition des matrices $G_V$ et $G_U$, la matrice $ 
\begin{pmatrix}
\begin{array}{c|c}
G_U & 0 \\
 \hline 
0 & G_V \\
\end{array} \\
\end{pmatrix} $ est de rang $k_U + k_V$. De plus les matrices $A$, $B$, $C$, $D$ étant diagonales, le déterminant de la matrice $\begin{pmatrix}
\begin{array}{c|c}
A & C \\
 \hline 
B & D \\
\end{array} \\
\end{pmatrix}$
est le produit des $(a_id_i - b_ic_i)$ pour $i \in \llbracket 1, n/2\rrbracket$, et donc non-nul par définition des vecteurs $a,b,c,d$. On a donc bien $k = k_U + k_V$. \\
On remarque aussi que $GH^T = 0$ et que $H$ est de rang plein par le même raisonnement que précédemment, ce qui conclut la preuve.
\end{proof}



\subsection{Le principe de signature}

Notre schéma de signature utilisera donc les codes $(U,U+V)$-généralisés et la fonction syndrome comme fonction à sens unique, sous l'hypothèse de la difficulté de résoudre le problème du décodage. \\

\noindent Nous allons définir la notion de fonctions GPV en moyenne (GPVM). Pour cela, introduisons d'abord la notion de distance statistique.

\begin{defi}
Soient $X$ et $Y$ deux variables aléatoires à valeurs dans le même espace $\epsilon$. 
Soient $\mathcal{D}_X$ et $\mathcal{D}_Y$ leurs distributions respectives. On définit la distance statistique entre ces deux distributions comme :
$$ \rho(\mathcal{D}_X,\mathcal{D}_Y) := \frac{1}{2} \sum_{x \in \epsilon} |\mathcal{D}_X(x) \mathcal{D}_Y(x)|.$$
\end{defi}

\begin{defi} (Fonctions GPVM). On appelle fonction GPV en moyenne une paire d'algorithmes (\verb|Trapdoor|,\verb|InvertAlg|) ainsi qu'un triplet de fonctions ($n(\lambda),k(\lambda),\omega(\lambda)$) en fonction d'un paramètre de sécurité $\lambda$, tels que :
\begin{itemize}
\item \verb|Trapdoor| est un algorithme probabiliste et polynomial en $1^\lambda$ et renvoyant le couple $(H,T)$ où $H \in \F_q^{(n-k) \times n}$ de rang $n-k$ et $T$ est la trappe associée.
\item \verb|InvertAlg| est un algorithme probabiliste et polynomial prenant en entrée la trappe $T$ et un syndrôme $s \in \F_q^{n-k}$, et renvoyant $e \in \F_q^{n}$ de poids $\omega$ tel que $eH^T = s$.
\end{itemize}
De plus, pour \textit{presque toutes} matrice $H$ renvoyée par \verb|Trapdoor|, la fonction est :
\begin{enumerate}
\item bien distribuée : \\
$\rho(eH^T,s) \in \text{negl}(\lambda)$ où $e$ est pris uniformément dans l'ensemble des mots de poids $\omega$ et de longueur $n$ et $s$ est pris uniformément dans $\F_q^{n-k}$. 
\item sans fuite d'information \textit{en moyenne} : \\
$ \rho(\verb|InvertAlg|(s,T),e) \in \text{negl}(\lambda)$ où $e$ est pris uniformément dans l'ensemble des mots de poids $\omega$ et de longueur $n$ et $s$ est pris uniformément dans $\F_q^{n-k}$. 
\item \`A sens unique sans la trappe : \\
Pour tout algorithme probabiliste polynomial $\mathcal{A}$, on a 
$$\mathbb{P}(\mathcal{A}(H,s) = e \;| \;eH^T = s) \in \text{negl}(\lambda).$$
\end{enumerate}
C'est une définition relaxée des fonctions GPV.
\end{defi}

\noindent Nous générons notre paire de clés publique et privée comme :
$$ (pk, sk) = (H,T) \leftarrow \verb|Trapdoor|(\lambda)$$
Plus précisément, nous aurons donc pour clé publique la matrice de parité du code $(U,U+V)$, notée $H$, et pour clé privée, les matrices de parité des codes $U$ et $V$, respectivement notées $H_U$ et $H_V$.\\
Nous pouvons maintenant définir notre système de signature.
\begin{multicols}{2}
\begin{flushleft}
$\verb|Sign|^{sk}(s)$:\\
	$\quad$ e $\leftarrow$  \verb|InvertAlg|(s,T) \\
	$\quad$ \verb|renvoie| e
\end{flushleft}
\begin{flushleft}
$\verb|Verify|^{pk}(s,e')$: \\
	$\quad \verb| Si | e'H^T = s \verb| et | |e'| = \omega $ \\
	$\quad \quad$ \verb|renvoie 1| \\
	$\quad$ \verb|renvoie 0|
\end{flushleft}
\end{multicols}

EXPLIQUER POURQUOI LE SYSTÈME FONCTIONNE.

\subsection{Le décodage avec trappe}

En partant de l'hypothèse que la matrice de parité $\mathbf{H}$ du code $(U,U+V)$-généralisé ressemble à une matrice aléatoire, la difficulté de créer une fausse signature sans connaître la trappe $\mathbf{T}$ est exactement celle de résoudre le problème du décodage d'un code aléatoire, que l'on sait difficile. Nous allons expliciter dans cette section l'algorithme d'inversion de la fonction syndrome connaissant la trappe, et discuter de sa difficulté en fonction du poids $\omega$ de $\e$. \\

\noindent Notons $\mathcal{S}_{\omega,n}$ l'ensemble des mots de poids $\omega$ et de longueur $n$. On notera dans la suite $\mathcal{S}_{\omega}$ s'il n'y a pas d’ambiguïté sur la longueur. On rappelle que l'algorithme \verb|InvertAlg| cherche à inverser la fonction syndrome : 
$$\begin{array}{ccccc}
f_{\omega,\mathbf{H}} & : & \mathcal{S}_{\omega,n} & \to & \F_q^{n-k} \\
 & & \mathbf{e} & \mapsto & \mathbf{eH^T} \\
\end{array}$$

\noindent On rappelle que la fonction $f_{\omega,\mathbf{H}}$ avec $\mathbf{H} \in \F_q^{(n-k)\times n}$ s'inverse génériquement si $\omega \in \{\omega_{easy}^-,\omega_{easy}^+\}$, où :
$$ \omega_{easy}^- := \frac{q-1}{q}(n-k) \qquad \text{ et }\qquad  \omega_{easy}^+ := k + \frac{q-1}{q}(n-k).$$




\noindent On rappelle aussi que la fonction $f_{\omega,\mathbf{H}}$ admet un inverse pour toute entrée $s \in \F_q^{n-k}$ si $\omega \in \{\omega^-,\omega^+\}$, où :
$$\omega^- := \min\left\{\omega\in \llbracket0,n\rrbracket , \dbinom{\omega}{n}(q-1)^{\omega} \geq q^{n-k}\right\} $$
$$\omega^+ := \max\left\{\omega\in \llbracket0,n\rrbracket , \dbinom{\omega}{n}(q-1)^{\omega} \geq q^{n-k}\right\}$$


\noindent Nous voulons donc un moyen d'inverser la fonction syndrome pour $\omega \in \llbracket\omega_{UV}^-,\omega_{UV}^+\rrbracket$ avec $\omega_{UV}^-$ et $\omega_{UV}^+$ tels que :

$$\llbracket\omega_{easy}^-,\omega_{easy}^+\rrbracket \subsetneq \llbracket\omega_{UV}^-,\omega_{UV}^+\rrbracket \subset  \llbracket\omega^-,\omega^+\rrbracket$$


\noindent Afin d'expliciter le décodage, introduisons la fonction :

$$\begin{array}{ccccc}
\varphi_{\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}} & : & \F_q^{n/2} \times  \F_q^{n/2} & \to & \F_q^{n/2} \times  \F_q^{n/2} \\
 & & (\mathbf{x} , \mathbf{y}) & \mapsto &  (\mathbf{a}.\mathbf{x} + \mathbf{b}.\mathbf{y}, \mathbf{c}.\mathbf{x} + \mathbf{d}.\mathbf{y}) \\
\end{array}$$

\noindent Si cette fonction respecte les conditions sur les vecteurs $\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}$ définies dans la définition \ref{UV-normalise}, on dit qu'elle est UV-normalisée. Dans ce cas on peut vérifier qu'elle est bijective d'inverse :

$$\begin{array}{ccccc}
\varphi^{-1}_{\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}} & : & \F_q^{n/2} \times  \F_q^{n/2} & \to & \F_q^{n/2} \times  \F_q^{n/2} \\
 & & (\mathbf{x} , \mathbf{y}) & \mapsto &  (\mathbf{d}.\mathbf{x} - \mathbf{b}.\mathbf{y}, -\mathbf{c}.\mathbf{x} + \mathbf{a}.\mathbf{y}) \\
\end{array}$$

\noindent Ainsi, pour chaque vecteur $\mathbf{e}$ de $\F_q^n$, on peut associer deux vecteurs $\mathbf{e_U}$ et $\mathbf{e_V}$ de $\F_q^{n/2}$ tels que 
$$ (\mathbf{e_U},\mathbf{e_V}) = \varphi^{-1}_{\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}}(\mathbf{e}).$$

\begin{propo} Inverser $f_{\omega,\mathbf{H}}$ pour un certain $\mathbf{s} \in F_q^{n-k}$ est équivalent à trouver $\mathbf{e} \in \F_q^n$ tel que:
$$ \mathbf{e}_U\mathbf{H}_U^T = \mathbf{s}^U \qquad \text{et} \qquad \mathbf{e}_V\mathbf{H}_V^T = \mathbf{s}^V $$
où $\mathbf{s} = (\mathbf{s}^U, \mathbf{s}^V)$ avec $\mathbf{s}^U \in \F_q^{n/2-k_U}$ et $\mathbf{s}^V \in \F_q^{n/2-k_V}$.
\end{propo}

\begin{proof}[Preuve]
Nous voulons montrer que le syndrome de $\e$ vaut $(\e_U\mathbf{H}_U^T,\e_V\mathbf{H}_V^T)$.
Remarquons d'abord que nous avons

\begin{equation*}
\begin{split}
e &= \varphi(\e_U,\e_V)\\
&= (\mathbf{a}.\e_U + \mathbf{b}.\e_V,\ \mathbf{c}.\e_U + \mathbf{d}.\e_V)\\
&= (\e_U A + \e_V B,\ \e_U C + \e_V D)\\
\end{split}
\end{equation*}

\noindent avec $A$, $B$, $C$, $D$ les matrices diagonales définie auparavant.\\
De plus, notons que

\vspace{0.1in}
$$ 
H^T :=
\begin{pmatrix}
\begin{array}{c|c}
D^TH_U^T & -C^TH_V^T \\
 \hline 

-B^TH_U^T & A^TH_V^T \\
\end{array} \\
\end{pmatrix}
$$
\vspace{0.1in}

\noindent Nous avons $e\mathbf{H}^T = \s = (\s^U,\ \s^V)$. Ainsi, en calculant le syndrome de $\e$ à l'aide des expressions précédentes, nous obtenons :

\begin{equation*}
\begin{aligned}
&\left\{
\begin{split}
(\e_UA+\e_VB)D^T\mathbf{H}_U^T - (\e_UC+\e_VD)B^T\mathbf{H}_U^T &= \s^U \\ 
-(\e_UA+\e_VB)C^T\mathbf{H}_V^T + (\e_UC+\e_VD)A^T\mathbf{H}_V^T &= \s^V \\
\end{split}
\right.\\
\iff
&\left\{
\begin{split}
\e_U(AD^T - CB^T)\mathbf{H}_U^T + \e_V(BD^T-CB^T)\mathbf{H}_U^T &= \s^U \\ 
\e_U(CA^T - AC^T)\mathbf{H}_V^T + \e_V(DA^T-BC^T)\mathbf{H}_V^T &= \s^V \\ 
\end{split}
\right.
\end{aligned}
\end{equation*}\

\noindent Comme $A$, $B$, $C$ et $D$ sont des matrices diagonales, elles sont égales à leur transposées et leur produit est commutatif. Nous avons alors :
\begin{equation*}
\left\{
\begin{split}
\e_U(AD - BC)\mathbf{H}_U^T &= \s^U\\
\e_V(AD - BC)\mathbf{H}_V^T &= \s^V\\
\end{split}
\right.
\end{equation*}

\noindent De plus, nous avons posé, lors de la définition des codes $(U,U+V)$-généralisées que nous utilisons que $\forall i \in \llbracket 1,n/2\rrbracket \quad a_id_i - b_ic_i = 1$, donc $AD-BC = I_{n/2}$. Nous obtenons donc bien le résultat souhaité, c'est-à-dire :

\begin{equation*}
\left\{
\begin{split}
\e_U\mathbf{H}_U^T &= \s^U\\
\e_V\mathbf{H}_V^T &= \s^V\\
\end{split}
\right.
\end{equation*}

\end{proof}

\noindent Ainsi, on on aura : \\
\begin{flushleft}
\leftskip=2cm
\verb|InvertAlg|$(\mathbf{s},\mathbf{T}) : $\\
$\qquad (\mathbf{s}_U, \mathbf{s}_V) = s $\\
$\qquad \mathbf{e}_U = \verb|DECODE_U|(\mathbf{s}_U) $\\
$\qquad \mathbf{e}_V = \verb|DECODE_V|(\mathbf{s}_V)$ \\
$\qquad \verb|renvoie | \varphi_{\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}}(\mathbf{e_U},\mathbf{e_V})$ \\
\leftskip=0cm
\vspace{0.1in}

\end{flushleft}
Si l'on choisit un algorithme générique pour \verb|DECODE_U| et \verb|DECODE_V|, alors nous obtiendrons un vecteur $\mathbf{e}$ de poids $\omega \in \{\omega_{easy}^-,\omega_{easy}^+\}$. Nous allons montrer comment utiliser les propriétés des codes $(U,U+V)$-généréralisés pour permettre un décodage hors de cet intervalle. 

\begin{remarque} Pour tout $\mathbf{e} = \varphi_{\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}}(\mathbf{e_U},\mathbf{e_V})$, on a pour tout $i \in \llbracket 1,n/2\rrbracket$ :
\begin{center}

$\left \{
\begin{array}{rcl}
a_i\mathbf{e}_U(i) + b_i\mathbf{e}_V(i) &=& \mathbf{e}(i) \\
c_i\mathbf{e}_U(i) + d_i\mathbf{e}_V(i) &=& \mathbf{e}(i+n/2) 
\end{array}
\right.$
\end{center}

\noindent Choisir la valeur de $\mathbf{e}_U$ en fonction de la valeur de $\mathbf{e}_V$ nous permettra donc d'influer sur le poids de $\mathbf{e}$. On aura alors :

\begin{flushleft}
\leftskip=2cm
\verb|InvertAlg|$(\mathbf{s},\mathbf{T}) : $\\
$\qquad (\mathbf{s}_U, \mathbf{s}_V) = s $\\
$\qquad \mathbf{e}_V = \verb|DECODE_V|(\mathbf{s}_V)$ \\
$\qquad \mathbf{e}_U = \verb|DECODE_U|(\mathbf{s}_U, \mathbf{e}_V) $\\
$\qquad \verb|renvoie | \varphi_{\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}}(\mathbf{e_U},\mathbf{e_V})$ \\
\leftskip=0cm
\vspace{0.1in}
\end{flushleft}

\end{remarque}

\begin{propo} Soit $\mathbf{e}_V$ une sortie de \verb|DECODE_V|. Soit \verb|DECODE_U| un algorithme prenant en entrée $\mathbf{s}_U$ et $\mathbf{e}_V$ et renvoyant $\mathbf{e}_U$ tel que $\mathbf{e}_U\mathbf{H}_U^T = \mathbf{s}^U$ et tel que pour $k_U$ positions de $\mathbf{e}_U$ 
\begin{center}
$\left \{
\begin{array}{rcl}
a_i\mathbf{e}_U(i) + b_i\mathbf{e}_V(i) &\neq& 0 \\
c_i\mathbf{e}_U(i) + d_i\mathbf{e}_V(i) &\neq& 0
\end{array}
\right.$
\end{center}
Alors $\mathbf{e} = \varphi_{\mathbf{a},\mathbf{b},\mathbf{c},\mathbf{d}}(\mathbf{e_U},\mathbf{e_V})$ a au moins $2k_U$ coordonnées non nulles. De plus les $n-k_U$ autres coordonnées sont uniformément distribuées sur $\F_q$. \\
On a alors 
$$ \mathbb{E}(|\mathbf{e}|) = \frac{q-1}{q}n + \frac{2k_U}{q} $$
et on peut alors espérer obtenir en temps polynomial des erreurs de poids:
\begin{center}
$\omega^+_{UV} = $
$\left \{
\begin{array}{rcl}
&\frac{q-1}{q}n + \frac{2k}{q} & \;\; \text{ si } k \leq \frac{n}{2} \\
&n & \quad \text{sinon}
\end{array}
\right.$
\end{center}
\end{propo}

\begin{proof}[Preuve]
La preuve étant similaire à celle de la proposition \ref{Wuv-}, nous ne la détaillerons pas ici.\\
\end{proof}


\begin{propo}\label{Wuv-} Soit $\mathbf{e}_V$ une sortie de \verb|DECODE_V|. Soit \verb|DECODE_U| un algorithme prenant en entrée $\mathbf{s}_U$ et $\mathbf{e}_V$ et renvoyant $\mathbf{e}_U$ tel que $\mathbf{e}_U\mathbf{H}_U^T = \mathbf{s}^U$ et tel que pour $k_U$ positions de $\mathbf{e}_U$ 
\begin{equation}\label{syst petit poid}
\left \{
\begin{array}{rcl}
a_i\mathbf{e}_U(i) + b_i\mathbf{e}_V(i) &=& 0 \\
c_i\mathbf{e}_U(i) + d_i\mathbf{e}_V(i) &=& 0
\end{array}
\right.
\end{equation}
On peut alors espérer obtenir en temps polynomial des erreurs de poids:
\begin{center}
\begin{equation} 
\omega^-_{UV} = 
\left \{
\begin{array}{rcl}
&\frac{q-1}{q}(n-2k) & \;\; \text{ si } k \leq \frac{n}{2q} \\
&\frac{2(q-1)^2}{(2q-1)q}(n-k) & \quad \text{sinon}
\end{array}
\right.
\end{equation}
\end{center}
\end{propo}

\begin{proof}[Preuve]
Il n'existe de solution au système (\ref{syst petit poid}) que si $\e_V(i)=0$ car pour tout $i$ on a $a_id_i -b_ic_i \neq 0$. De ce fait, à l'inverse du cas où nous souhaitions des erreurs de gros poids, l'ensemble d'indices où l'on peut gagner deux fois est réduit à $n/2 - |\e_V|$. De ce fait le poids minimal que nous pouvons espérer pour $\e_V$ est $|\e_V|_{min} := \frac{q-1}{q}(n/2-k_V)$. Ainsi :
\begin{itemize}
\item Si $k_U \leq n/2 - |\e_V|_{min}$, nous pouvons obtenir des erreurs $e$ telles que :
	\begin{itemize}
	\item $2k_U$ coordonnées sont nulles.
	\item Les autres coordonnées sont uniformément distribuées.
	\end{itemize}
\item Sinon, nous pouvons obtenir des erreurs $e$ telles que :
	\begin{itemize}
	\item $2(n/2 - |\e_V|_{min})$ sont nulles.
	\item $k_U - (n/2 - |\e_V|_{min})$ autres coordonnées sont nulles tandis que $k_U - (n/2 - |\e_V|_{min})$ sont non nulles.
	\item Les autres coordonnées sont uniformément distribuées.
	\end{itemize}
\end{itemize} 


\noindent Nous allons maintenant détailler le calcul de $\omega_{UV}^-$. Supposons que notre code a une dimension $k = k_U + k_V$ et rappelons que nous cherchons à obtenir une erreur de la forme $\e = (\e_U,\e_U + \e_V)$ avec $k_U$ coordonnées nulles.\\
\noindent On commence par décoder $\e_V$ avec un algorithme générique. 
Nous pouvons donc découper ce dernier en fonction de son poids $|\e_V|$ de la façon suivante : $ \e_V = (0, \e_V'')$ avec $0 \in \F_q^{n/2-|\e_V|}$ et $\e_V''(i) = 0 \;\forall i$.
On écrit alors $\e_U$ à l'aide du même découpage : $\e_U = (\e_U', \e_U'')$ avec $\e_U' \in \F_q^{n/2-|\e_V|}$ et $\e_U'' \in \F_q^{|\e_V|}$.\\

\noindent Nous allons dans cette démonstration, montrer la deuxième partie de l'égalité. Supposons que $k_U \geq n/2 - |\e_V|$, ce qui signifie que nous voulons choisir plus de coordonnées nulles qu'en dispose $\e_U'$. 
Nous allons donc, lors de notre algorithme, fixer toutes les coordonnées de $\e_U'$ à zéro, et il nous restera à annuler $k_U - (n/2 -|\e_V|)$ coordonnées de $\e_U''$.
Nous obtenons alors 
\begin{equation*}
\begin{split}
&|\e_U| = \frac{q-1}{q}(n/2-k_U) \\
&|\e_U + \e_V| = k_U - n/2 + |\e_V| + \frac{q-1}{q}(n/2 -k_U)
\end{split}
\end{equation*}

En effet, comme nous annulons $k_U - n/2 + |\e_V|$ coordonnées de $\e_U''$ et que $\e_V''(i) = 1 \; \forall i$, ces mêmes coordonnées sont non nulles pour $\e_U'' + \e_V''$. Il reste alors $n/2 -k_U$ coordonnées qui seront non nulles si $\e_U''(i) \neq -\e_V''(i)$. \\
Nous avons alors une erreur $\e$ de poids :
$$ |\e| =  |\e_U| + |\e_U + \e_V| = 2\frac{q-1}{q}(n/2-k_U) + k_U - n/2 - |\e_V|$$

\noindent Ici, nous supposons que notre algorithme de décodage nous renvoie $\e_V$ de poids minimum, nous avons donc $$|\e_V| = |\e_V|_{min} = \frac{q-1}{q}(n/2-k_V) = \frac{q-1}{q}(n/2-k+k_U)$$

\noindent Ainsi nous obtenons une erreur dont le poids est :
\begin{equation*}
\begin{split}
|\e| &= 2\frac{q-1}{q}(n/2-k_U) + k_U - n/2 - \frac{q-1}{q}(n/2-k+k_U)\\
 &= n\left(  \frac{3}{1}\frac{q-1}{q}-\frac{1}{2}\right) + k_U \left(1 - \frac{q-1}{q} \right) - k \frac{q-1}{1}\\
  &= n\frac{2q-3}{2q} + k_U\frac{1}{q}+ k \frac{1-q}{1}\\
\end{split}
\end{equation*}

Pour minimiser le poids de $\e$ nous devons donc prendre $k_U$ le plus petit possible. Comme nous avons supposé avoir $k_U \geq n/2 - |\e_V|$, nous prenons donc 
\begin{equation*}
\begin{split}
k_U &= \frac{n}{2} - |\e_V|\\
&= \frac{n}{2} - \frac{q-1}{q}(n/2-k+k_U)\\
&= \frac{n}{2q} + k\frac{q-1}{q} -k_U\frac{q-1}{q}\\
\end{split}
\end{equation*}
Ce qui nous donne alors $k_U = \frac{n}{2(2q-1)} + k\frac{q-1}{q}$.\\

Par définition, nous avons $k_U \leq k$, nous pouvons se servir de cette conditions pour minorer $k$ :
$$\frac{n}{2(2q-1)} + k\frac{q-1}{q} \leq k$$
$$\frac{n}{2(2q-1)} \leq k\frac{q-1}{2q-1}$$
$$\frac{n}{2} \leq k$$

Nous nous trouvons donc bien dans le deuxième cas : lorsque $k \geq n/2$. Finalement, nous pouvons calculer le poidsde $\e$ minimum :
\begin{equation*}
\begin{split}
|\e| &= n\frac{2q-3}{2q} + \left(\frac{n}{2(2q-1)} + k\frac{q-1}{q}\right)\frac{1}{q}+ k \frac{1-q}{1}\\
&= n\left(\frac{2q-3}{2q} + \frac{1}{2(2q-1)q}\right) + k\left(\frac{q-1}{(2q-1)q}+ \frac{1-q}{1}\right)\\
&= n\;\frac{4q^2-8q+4}{2q(2q-1)} + k\;\frac{-2q^2+4q-2}{(2q-1)q}\\
&= n\;\frac{2(q-1)^2}{q(2q-1)} + k\;\frac{-2(q-1)^2}{(2q-1)q}\\
&= \frac{2(q-1)^2}{q(2q-1)}(n-k)\\
\end{split}
\end{equation*}

\noindent Nous avons donc bien le résultat souhaité lorsque $ k \geq \frac{n}{2q}$. La démonstration du cas où $k \leq \frac{n}{2q}$ étant équivalente, nous ne la ferons pas ici.\\

\end{proof}

\noindent On récapitule les différents cas dans la figure \ref{graphique ratio}. \\

\begin{figure}[h]
\begin{center}
\includegraphics [scale=0.4]{include/graph_ratio_w.png}
\end{center}
\caption{\small Comparaison des distances $w/n$ avec et sans trappe en fonction du rendement.}
\label{graphique ratio}
\end{figure}

\noindent La connaissance de la trappe apporte donc bien un avantage puisqu'elle permet un décodage pour des erreurs de poids ne permettant pas de décodage générique. 

\begin{remarque} En pratique, notre système de signature comporterait une fonction de hachage. Ainsi, la fonction de signature prendrait en entrée un mot $m$ quelconque. Puis un aléa $r$ serait pris aléatoirement dans $\{0,1\}^{\lambda_0}$. On aurait $s \leftarrow \verb|Hash| (m,r)$, qui serait alors signé. \verb|Sign| renvoit alors le couple $(e,r)$. \\
La fonction \verb|Verify| prend en entrée $(m,(\e',r))$, récupère $s \leftarrow \verb|Hash| (m,r)$ puis fait ses vérifications. Ainsi un attaquant ne peut pas falsifier une signature à partir d'un vecteur $e$ choisi aléatoirement dans $S_{\omega}$ et en récupérant $s = e\mathbf{H}^T$. Sans la fonction de hachage, il aurait alors signé $s$.
\end{remarque}

\subsection{Implémentation et choix de paramètres}
TODO \\
\section{Uniformisation des signatures et syndromes}

\subsection{Une fuite d'information}
Afin d'assurer la sécurité du système, il est nécéssaire que les $\mathbf{e} \in f_{w,\mathbf{H}}^{-1}(\mathbf{s})$ ne révèlent pas d'information sur la structure du code (U,U+V)-généralisé utilisé. \\
Or, si la sortie $\mathbf{e_V}$ de \verb|DECODE_V| n'est pas uniforme, alors des corrélations entre les coordonnées $\mathbf{e}_i$ et $\mathbf{e}_{i+n/2}$ du vecteur $\mathbf{e}$. \\
Par exemple, prenons le cas où $q=3$, et où pour tout $i \in \{1,n/2\}$, $a_i = c_i = d_i = 1$ et $b_i = 0$, et où \verb|DECODE_V| est l'algorithme de Prange. \\
On a alors pour tout $\mathbf{e} = (\mathbf{e_U},\mathbf{e_U}+\mathbf{e_V})$
$$ |\mathbf{e_V}| = \# \; \{1  \leq i \leq n/2 \;|\; e_i \neq e_{i+n/2}\}$$

\begin{propo}
Si le vecteur $\mathbf{e_V}$ est obtenu par l'algorithme de Prange, alors il est de poids moyen $\frac{2}{3}(\frac{n}{2}-k_V)$.
\end{propo}



Alors, pour tout $i \ in \{1,n/2\}$, on a :
$$ \mathbb{P}(\mathbf{e}_i \neq \mathbf{e}_{i+n/2}) = \frac{2}{3(n/2)}(n/2-k_V)(1+o(1))$$

\noindent En revanche, pour les autres paires $(i,j)$, on a :
$$ \mathbb{P}(\mathbf{e}_i \neq \mathbf{e}_{j}) = \frac{4wn - 3w^2-w}{n(n-1)}$$

Ces deux probabilités n'ont donc aucune raison d'être égales. On a donc une fuite d'information. En effet, dans la pratique et afin de cacher la structure, on effectue une permutation sur les coordonnées de $\mathbf{e}$ lors de la signature. Si un attaquant récupère suffisemment de signatures, il pourra donc en analysant la fréquence des $\mathbf{e}_i \neq \mathbf{e}_j$ retrouver cette permutation. Il est donc nécéssaire pour la sécurité du schéma de s'assurer de l'uniformité des sorties de l'algorithme \verb|sign|.

\subsection{La méthode du rejet}
Afin de s'assurer un $\mathbf{e}$ uniforme dans son ensemble, nous allons :
\begin{itemize}
\item choisir $\mathbf{e}_V$ de façon a ce qu'il soit uniforme dans son ensemble 
\item mettre des conditions de rejet sur $\mathbf{e}_U$ en fonction du poids de $\mathbf{e}_V$ afin de supprimer le biais sur l'ensemble 
$$ m_1(x) := \# \; \{1  \leq i \leq n/2 \;;\; |(x_i, x_{i+n/2})| = 1\}$$
\end{itemize}
Avant d'expliciter nos algorithmes, il est nécéssaire d'introduire quelques notations et définitions. \\

\begin{nota} On notera :
\begin{itemize}
\item $\mathbf{e}^{unif}$ la variable aléatoire tirée uniformément dans l'ensemble $S_{w,n}$
\item $\mathbf{e}_V^{unif}$ la variable aléatoire tirée uniformément dans les mots de $\F_q^{n/2}$ 
\item $\mathbf{e}_U^{unif}$ la variable aléatoire tirée uniformément dans les mots de $\F_q^{n/2}$ conditionné au vecteur $\e_V^{unif}$
\end{itemize}
\end{nota}


\begin{defi} (uniforme en poids et $m_1$-uniforme)
\begin{itemize}
\item \verb|DECODE_V| est dit uniforme en poids si ces sorties $\mathbf{e}_V$ sont telles que $\mathbb{P}(\mathbf{e}_V)$ n'est fonction que du poids de $\mathbf{e}_V$ quand $\mathbf{s}^V$ est tiré uniformément dans son ensemble.
\item \verb|DECODE_U| est dit $m_1$-uniforme si ces sorties $\mathbf{e}_U$ sont telles que $\mathbb{P}(\mathbf{e}_U\; |\;  \mathbf{e}_V)$ n'est fonction que du poids de $\mathbf{e}_V$ et de $m_1(\varphi(\mathbf{e}_U,\mathbf{e}_V))$.
\end{itemize}
\end{defi}

\begin{lemme} Soit $\mathbf{e}$ la sortie de \verb|InvertAlg| avec $\mathbf{s}_U$ et $\mathbf{s}_V$ choisis uniformément dans leurs ensembles. Soit \verb|DECODE_V| uniforme en poids et \verb|DECODE_U| $m_1$-uniforme. Si pour tout $y$ et $z$ 
$$|\mathbf{e}_V| \sim |\mathbf{e}_V^{unif}|\quad \text{et} \quad\mathbb{P}(m_1(\mathbf{e}) = z\; |\; |\mathbf{e}_V| = y) = \mathbb{P}(m_1(\mathbf{e}^{unif}) = z\; |\; |\mathbf{e}_V^{unif}| = y)$$
Alors
$$ \mathbf{e} \sim \mathbf{e}_V^{unif}.$$
\end{lemme}

\begin{proof}[Preuve]\
Nous allons montrer qu'avec les hypothèses précédentes nous avons $\forall x \in S_{\omega} \quad \mathbb{P}(\e = x) = \mathbb{P}(\e^{unif} = x)$.\\
Soit $x \in S_{\omega}$ :
\begin{equation*}
\begin{split}
\mathbb{P}(\e = x) &= \mathbb{P}(\e_U = x_U, \e_V = x_V)\\
&= \mathbb{P}(\e_U = x_U | \e_V = x_V)\mathbb{P}(\e_V = x_V)\\
\end{split}
\end{equation*}
Notre but étant de faire apparaître les expressions énoncées lors du lemme, nous allons exprimer ces deux probabilités en fonction de $|x_V| = y$ et de $m_1(x) = z$.
\begin{equation*}
\begin{split}
\mathbb{P}(\e_V = x_V) &= \mathbb{P}(\e_V = x_V, |\e_V| = y)\\
&= \left.\mathbb{P}(\e_V = x_V \right| |\e_V| = y)\ \mathbb{P}(|\e_V| = y)\\
&= \frac{\mathbb{P}(|\e_V| = y)}{n(y)}\\
\end{split}
\end{equation*}
avec $n(y) := \#\left\{ \left.\e \in \F_3^{n/2} \right| |e| = y\right\}$.
De la même façon, nous obtenons :
{\footnotesize \begin{equation*}
\begin{split}
\mathbb{P}(\e_U = x_U |\ |\e_V| = y) &= \left.\mathbb{P}(\e_U = x_U ,\ m_1(\e) = z\ \right|\ |\e_V| = y)\\
&= \left.\mathbb{P}(\e_U = x_U\ \right|\ m_1(\e) = z,\ |\e_V| = y)\mathbb{P}(m_1(\e) = z\ |\ |\e_V| = y)\\
&= \frac{\mathbb{P}(m_1(\e) = z\ |\ |\e_V| = y)}{n(y,z)}\\
\end{split}
\end{equation*}}
avec $n(y,z) := \#\left\{ \left.\e \in \F_3^{n/2} \right|\  m1(\e) = z\text{ et } |\e_V| = y \right\}$.\\

\noindent Ainsi nous obtenons
\begin{equation*}
\begin{split}
\mathbb{P}(\e = x) &= \frac{\mathbb{P}(m_1(\e) = z\ |\ |\e_V| = y)}{n(y,z)}\frac{\mathbb{P}(|\e_V| = y)}{n(y)}\\
 &= \frac{\mathbb{P}(m_1(\e^{unif}) = z\ |\ |\e_V^{unif}| = y)}{n(y,z)}\frac{\mathbb{P}(|\e_V^{unif}| = y)}{n(y)}\\
 &\quad \text{par les hypohèses}\\
 &= \mathbb{P}(\e_U^{unif} = x_U | \e_V^{unif} = x_V)\mathbb{P}(\e_V^{unif} = x_V)\\
 &= \mathbb{P}(\e^{unif} = x)\\
\end{split}
\end{equation*}
\end{proof}


\noindent Ainsi, pour que $\mathbf{e}$ soit uniformément distribué sur $S_\omega$, il suffit de choisir \verb|DecodeV| de façon à ce que ses sorties soient uniforment sur $\F_q^{n/2}$ puis d'ajouter une condition de rejet sur les sorties de \verb|DecodeU| de façon à ce que $m_1(\mathbf{e})$ conditionnée à $|\mathbf{e}_V|$ soit distribué comme $m_1(\mathbf{e}^{unif})$ conditionnée à $|\mathbf{e}_V^{unif}|$. \\
On peut alors introduire l'algorithme suivant :
\begin{algorithm}
	\caption{DecodeUV($\varphi, \s, \mathbf{H}_V, \mathbf{H}_U$)}
	\begin{algorithmic}[1]
   	 	\REQUIRE $\varphi$, $\s \in \F_q^{n-k}$ un syndrome, $\mathbf{H}_V \in \F_q^{(\frac{n}{2} - k_V) \times \frac{n}{2}}$, $\mathbf{H}_U \in \F_q^{(\frac{n}{2} - k_U) \times \frac{n}{2}}$
   	 	\ENSURE $\e = \varphi(e_U, e_V) \text{ avec } \e_U\mathbf{H}_U^T = \s^U \text{ et } \e_V\mathbf{H}_V^T = \s^V$
    	\STATE $\e_V \leftarrow \text {DecodeV}(\s^V,\mathbf{H}_V)$
    	\REPEAT 
    	\STATE $\e_U \leftarrow \text {DecodeU}(\varphi, \e_V, \s^U, \mathbf{H}_U)$
    	\STATE $\e \leftarrow \varphi(\e_U,\e_V)$
    	\UNTIL {$\text{rand}([0,1]) > \mathbf{r}_U(|\e_V|, m_1(\e))$}
    	\RETURN $\e$
    \end{algorithmic}
\end{algorithm}\


\noindent Avec :

\begin{equation*}
   \begin{split}
    r(s,t) &:= \frac{1}{M(t)}\frac{q^{unif}(s,t)}{q(s,t)}\\[.6cm]
    q(s,t) &:= \mathbb{P}(m_1(\mathbf{e})=s\;|\;|\mathbf{e}_V|=t)\\[.6cm]
    q^{unif}(s,t) &:= \mathbb{P}(m_1(\mathbf{e}^{unif})=s\;|\;|\mathbf{e}^{unif}_V|=t)\\[.6cm]
    M(t) &:= \max_{0 \leq s \leq t} \frac{q^{unif}(s,t)}{q(s,t)}\\[.6cm]
    \end{split}
\end{equation*}



Pour montrer que la sortie de notre algorithme \verb|DecodeUV| suit bien une distribution uniforme, énonçons le lemme suivant :

\begin{lemme}\label{rejet} Soient $X$ et $X^{unif}$ deux variables aléatoires à valeur dans un même ensemble $\mathcal{X}$ telles que :
\begin{itemize}
\item $X$ suit une distribution quelconque,
\item $X^{unif}$ suit une distribution uniforme.
\end{itemize} 
Pour tout $x \in \mathcal{X}$ on pose :
\begin{equation*}
   \begin{split}
    &r(x) := \frac{1}{M} \frac{\mathbb{P}(X^{unif}=x)}{\mathbb{P}(X=x)} \\[0.6cm]
    &M := \max_{y \in \mathcal{X}}\frac{\mathbb{P}(X^{unif}=y)}{\mathbb{P}(X=y)}\\[0.6cm]
    \end{split}
\end{equation*}
Alors la variable aléatoire $Y$ définie telle que pour tout $x$:
\begin{enumerate}
\item On tire $\theta$ uniformément dans l'intervalle $\llbracket 0,1 \rrbracket$,
\item Si $\theta \leq r(x)$, alors $Y$ prend la valeur $x$.
\item Sinon, $Y$ ne prends aucune valeur.
\end{enumerate}
Alors la variable $Y$ suit une loi uniforme. 
\end{lemme}


\begin{proof}[Preuve] Remarquons d'abord que pour tout $x$:
$$0 \leq r(x) = \frac{1}{M}\frac{\mathbb{P}(X^{unif}=x)}{\mathbb{P}(X=x)} = \Bigg(\inf_{y \ in \mathcal{X}}\frac{\mathbb{P}(X=y)}{\mathbb{P}(X^{unif}=y)}\Bigg)\frac{\mathbb{P}(X^{unif}=x)}{\mathbb{P}(X=x)}\leq 1$$
Donc les coordonnées de $r$ sont bien des probabilités.\\

\noindent On remarque aussi que,
\begin{equation*}
   \begin{split}
    \mathbb{P}(\text{ "y soit accepté" }) &:= \mathbb{P}(\; \theta \leq r(x)\quad \text{et} \quad X=x\;) \\[0.6cm]
    &:=  \mathbb{P}(\theta \leq r(x)) \times\mathbb{P}(X=x) \\[0.6cm]
    \end{split}
\end{equation*}

De plus, la probabilité que $Y = x$ est égale à la probabilité que $x$ ai été tiré et qu'il ai été accepté. 
\noindent Pour tout $x \in \mathcal{X}$, on a alors,
\begin{equation*}
   \begin{split}
    \mathbb{P}(Y=x) &:= \frac{ \mathbb{P}(\theta \leq r(x)) \times\mathbb{P}(X=x)}{\sum_{y \in \mathcal{X}} \mathbb{P}(\theta \leq r(y)) \times\mathbb{P}(X=y)} \\[0.6cm]
    &:=\frac{  r(x) \times\mathbb{P}(X=x)}{\sum_{y \in \mathcal{X}}  r(y) \times\mathbb{P}(X=y)}\qquad \Big(\text{car}\quad \mathbb{P}(\theta \leq r(x)) =  r(x)\Big)\\[0.6cm]
    &:=\frac{ \mathbb{P}(X^{unif}=x)}{M\times\sum_{y \in \mathcal{X}} 1/M \times\mathbb{P}(X^{unif}=y)}\qquad \Big(\text{car}\quad \sum_{y\in \mathcal{X}}\mathbb{P}(X^{unif}=y) = 1\Big)\\[0.6cm]
    &:= \mathbb{P}(X^{unif}=x) \\[0.6cm]
    \end{split}
\end{equation*}

\end{proof}



\noindent On peut alors énoncer la proposition suivante :


\begin{propo}
Si \verb|DecodeV| est uniforme en poids et si \verb|DecodeU| est $m_1$-uniforme, alors on a $\mathbf{e}\sim\mathbf{e}^{unif}$.
\end{propo}

\begin{proof}[Preuve]
Soit $\e_U'$ le vecteur obtenu à la sortie de la boucle dans \verb|DECODE_UV|.  Notons pour tout $i,j \in \llbracket 1,n \rrbracket$:
$$q'(i,j) := \mathbb{P}(m_1(\mathbf{e'}_U)=i\;|\;|\mathbf{e'}_V|=j)$$

\noindent On a alors par le lemme \ref{rejet}
$$q'(i,j) = q^{unif}(i,j)$$
De plus la sortie de l'algorithme \verb|DECODE_V| est uniforme, ce qui conclu la preuve.
\end{proof}

\subsection{Choix des algorithmes de décodage}
À FAIRE :\\
description explicite de \verb|DECODE_V| \\
description explicite de \verb|DECODE_U| \\

\begin{algorithm}
	\caption{DecodeU($\varphi, \e_V, \s^U, \mathbf{H}_U$)}
	\begin{algorithmic}[1]
		\STATE $t \leftarrow |\e_V|$
		\STATE $k_0 \hookleftarrow \mathcal{D}_U^t$
		\REPEAT
		\STATE $\mathcal{I} \leftarrow$ ensemble d'information de $\langle\mathbf{H}_U\rangle^\perp$
		\STATE $\mathcal{J} \subset \mathcal{I}$ de taille $k-d$ tel que $|\e_V|_\mathcal{J} = k_0$
		\STATE $x_U \hookleftarrow \{x\in\F_3^{n/2} | \forall j\in\J,  x_j \notin \{-\frac{b_i}{a_i}\e_{V_i}, -\frac{d_i}{c_i}\e_{V_i}\}\}$
		\STATE $\e_U \leftarrow \textsc{Prange }(\mathbf{H}_U, \s^U, \mathcal{I}, x_U)$
		\UNTIL {$|\varphi(\e_U,\e_V)| \neq \omega$}
		\RETURN $\e_U$
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
	\caption{DecodeV($\s^V$)}
	\begin{algorithmic}[1]
    	\STATE $c$ mot aléatoire du code $V$
    	\STATE $\s \leftarrow$ le syndrome $\s^V$ paddé avec des zéros
    	\STATE $\e_V \leftarrow \s + c$
    	\RETURN $\e_V$
    \end{algorithmic}
\end{algorithm}

EXPLIQUER POURQUOI ON AJOUTE LE $d$.\\
Avec $d\in [0,k_U]$, qui permettra de montrer que notre sortie est uniforme.

\subsection{Estimation du nombre de rejet}
Nous souhaitons estimer le nombre de rejets effectué dans notre algorithme \verb|DecodeUV| utilisant les algorithmes \verb|DecodeU|, \verb|DecodeV| définit précédemment. Pour cela, introduisons la définition suivante:

\begin{defi} (Bon ensemble).
Soient $d \leq k \leq n$, $\mathbf{H}\in\F_3^{(n-k)\times n}$ et $\varepsilon \subseteq \llbracket1,n\rrbracket$ de taile $k-d$. On dit que $\varepsilon$ est un bon ensemble pour $\mathbf{H}$ si $\mathbf{H}_{\overline{\varepsilon}}$ est de rang plein. Sinon, on dit que $\varepsilon$ est un mauvais ensemble.
\end{defi}


\noindent Pour estimer ce nombre de rejets moyen, nous allons comparer $\e$ la sortie de \verb|DecodeUV| avec $\e^{unif}$ une erreur aléatoire uniforme de poids $\omega$.\\
Nous savons que la sortie de \verb|DecodeV| est uniforme, nous allons donc étudier la sortie de \verb|DecodeU|.
Nous introduisons pour cela \verb|VarDecodeU| qui fonctionne de la même façon que \verb|DecodeU| quand $\J$ est un bon ensemble pour $\mathbf{H}$ et qui renvoie une erreur aléatoire selon la distribution $\D_U^t$ sur $\J$ dans le cas contraire.
Il n'y a donc aucune raison que la sortie soit une solution du problème de décodage lorsque $\J$ n'est pas un bon ensemble pour $\mathbf{H}$.
Nous pouvons facilement voir que \verb|VarDecodeU| est $m_1$-uniforme.\\
La sortie de l'algorithme \verb|DecodeUV| utilisant \verb|VarDecodeU| est alors uniforme, on la note $\e^{unif}$.\\

\begin{defi}\
\begin{itemize}
\item $J_{x_V,l}^{unif}$ et un ensemble choisi uniformément tel que $J_{x_V,l}^{unif}\subseteq \llbracket 1,n/2 \rrbracket$, il est de cardinal $k_U-d$ et $\#J_{x_V,l}^{unif}\cap Supp(x_V) = k_0$. 
\item $J_{x_V, l}^{\mathbf{H}_U}$ est défini de la même façon avec une contrainte supplémentaire : il fait parti des bons ensembles pour $\mathbf{H}_U$.
\end{itemize}
\end{defi}

\noindent Pour plus de simplicité, nous les noterons par la suite $J^{unif}$ et $J^{\mathbf{H}_U}$.\\

\noindent Pour pouvoir compter le nombre de rejets, nous allons avoir besoin des lemmes suivants dont les preuves sont en annexes.

\begin{lemme}\label{maj_dist_e_eunif}
Nous pouvons majorer la distance statistique entre la sortie de l'algorithme utilisant \verb|DecodeU| et celle de l'algorithme utilisant \verb|VarDecodeU| de la façon suivante :
$$ \rho\left(\e ,\e^{unif}\right) \leq \sum\limits_{x_V,l} \rho\left(J^{\mathbf{H}_U},J^{unif}\right)\mathbb{P}\left(k_0 = l, \e_V = x_V\right) $$ 
\end{lemme}


\begin{lemme}\label{esp}
L'espérance de la différence statistique entre l'ensemble $J^{unif}$ choisi uniformément et l'ensemble $J^{\mathbf{H}_U}$ choisi parmi les bons ensembles pour $\mathbf{H}$ peut être majorée comme suit :
$$ \mathbb{E}\left(\rho\left(J^{unif},J^{\mathbf{H}_U}\right)\right) \leq \frac{3^{-d}}{2} $$
\end{lemme}

\begin{lemme}\label{markov}(Inégalité de Markov).\\
Soit $Z$ une variable aléatoire supposée presque sûrement positive ou nulle, alors $$\forall a>0\quad \mathbb{P}(Z > a) \leq \frac{\mathbb{E}(Z)}{a}$$
\end{lemme}

\noindent Nous pouvons maintenant énoncer le théorème suivant :

\begin{thm}\label{rejet}
Soit $\e$ la sortie de \verb|DecodeUV| et $\e^{unif}$ la variable aléatoire uniformément distribuée parmi les mots de poids $\omega$. On a :
$$ \mathbb{P}\left(\rho(\e,\e^{unif})>3^{-d/2}\right) \leq \frac{3^{-d/2}}{2} $$
\end{thm}

\begin{proof}[Preuve]
\begin{equation*}
\begin{split}
\mathbb{P}&\big(\rho(\e,\e^{unif})>3^{-d/2}\big) \\
&\leq 3^{d/2}\mathbb{E}(\rho\left(\e,\e^{unif})\right) \quad \text{par l'inégalité de Markov}\\
&\leq 3^{d/2}\mathbb{E}\left(\sum\limits_{x_V,l} \rho\left(J^{\mathbf{H}_U},J^{unif}\right)\mathbb{P}\left(k_0 = l , \e_V = x_V\right)\right) \text{ d'après le lemme \ref{maj_dist_e_eunif}}\\
&\leq 3^{d/2}\left(\sum\limits_{x_V,l} \frac{3^{-d}}{2}\mathbb{P}\left(k_0 = l ,\e_V = x_V\right)\right) \quad \text{par \eqref{esp}}\\
&= 3^{d/2} \frac{3^{-d}}{2}\left(\sum\limits_{x_V,l}\mathbb{P}\left(k_0 = l ,\e_V = x_V\right)\right) \\
&= \frac{3^{-d/2}}{2}\\
\end{split}
\end{equation*}
\end{proof}


\noindent La probabilité d'avoir un rejet équivaut à la probabilité d'avoir une distance significative entre $\e$ et $\e^{unif}$. D'après le théorème \ref{rejet}, nous voyons donc qu'en faisant augmenter $d$, nous serons en mesure d'effectuer très peu de rejets.


\section{Sécurité du schéma}
Pour montrer la sécurité du schéma, nous allons dans un premier temps montrer que sous l'hypothèse que la matrice de parité du code considéré est difficile à distinguer d'une matrice aléatoire, le schéma est sûr au sens EUF-CMA.\\
Nous montrerons ensuite qu'il est effectivement difficile de distinguer notre matrice de parité permutée d'une matrice aléatoire. \\

\subsection{Sécurité EUF-CMA}
Nous allons montrer que le schéma est sûr au sens EUF-CMA (Existential Unforgeability under Chosen Message Attacks). Pour cela nous ferons une réduction au problème DOOM.
\subsubsection{Définitions}

Soit $\mathcal{A}$ un adversaire ayant accès à $N_{sign}$ signatures de son choix. Soit les trois algorithmes suivants :


\begin{algorithm} [h]
	\caption{Init($\lambda$)}
	\begin{algorithmic}[1]
    	\STATE $(pk,sk) \Longleftarrow$ Gen$(1^\lambda)$ 
    	\STATE $\mathbf{H}_{pk} \Longleftarrow pk$
    	\STATE $(\varphi,\mathbf{H}_{U},\mathbf{H}_{V})\Longleftarrow sk$
    	\RETURN $\mathbf{H}_{pk}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
	\caption{Sign($s$)}
	\begin{algorithmic}[1]
    	\STATE $\e \leftarrow \mathcal{D}_{\varphi,\mathbf{H}_{U},\mathbf{H}_{V}}(\s)$
    	\RETURN $\e$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
	\caption{Fin($(s,e)$)}
	\begin{algorithmic}[1]
    	\RETURN $(\mathbf{e}\mathbf{H}_{pk}^T = s) \land (|\mathbf{e}| = \omega)$
    \end{algorithmic}
\end{algorithm}

\noindent Le jeu EUF-CMA se déroule comme suit. $\mathcal{A}$ fait appel à \verb|Init|. Il peut ensuite faire $N_{sign}$ requêtes à \verb|sign|. Le jeu est dit réussi si $\mathcal{A}$ est capable de donner $(s,e)$ accepté par \verb|Fin| et tel que $s$ n'est jamais été demandé à \verb|Sign|. \\
On définit alors le succès EUF-CMA comme :
$$Succ^{EUF-CMA}_{Wave}(t,N_{sign}) := \max_{\mathcal{A};|A|\leq t}(\mathbb{P}(\mathcal{A}\text{ réussit le jeu EUF-CMA de Wave})).$$
Le protocole est alors sûr au sens EUF-CMA si ce succès est négligeable. \\

\noindent Nous souhaitons donc montrer que notre système est sûr au sens EUF-CMA. Pour cela, nous allons dans la section suivante majorer ce succès par rapport au succès d'un problème connu, le problème DOOM.


\subsubsection{Réduction au problème DOOM}
\textbf{Le problème DOOM.} Soient des paramètres $(n,q,k,\omega,N)$, où $N$ est un entier. \\

\leftskip=1cm
\noindent \textbf{I :} $\mathbf{H}$ une matrice uniforme de $\F_q^{(n-k)\times n}$ et $(\mathbf{s}_1,...,\mathbf{s}_N)$ une liste de $N$ syndromes. 

\noindent \textbf{Q :} Décoder l'un des syndromes à la distance $w := \lfloor \omega n \rfloor$. \\

\leftskip=0cm

\noindent On définit alors le succès de DOOM comme :
$$Succ^{DOOM(n,q,k,N)}(t) := \max_{\mathcal{A};|A|\leq t}(\mathbb{P}(\mathcal{A}(\mathbf{H},\mathbf{s}_1,...,\mathbf{s}_n)=\mathbf{e}\text{ tel que }$$
$$ \mathbf{eH}^T = \mathbf{s}_j \text{ pour un certain } j \in \{1,...,N\})).$$

\noindent La réduction à ce problème est naturelle pour un schéma de signature. Il suffit de s'assurer que l'on peut considérer notre matrice de parité comme une matrice d'un code aléatoire. Ainsi, si $\mathcal{A}$ arrive à créer une signature valide sans connaître l'algorithme de décodage $\mathcal{D}_{\varphi,\mathbf{H}_{U},\mathbf{H}_{V}}$, il a bien résolu le problème du décodage. Puisqu'on a montré préalablement que les sorties $\e$ de \verb|sign| sont uniformément distribuées sur $S_{\omega}$, la connaissance des $N_{sign}$ couples $(\s,\e)$ ne donnent pas d'avantage à $\mathcal{A}$. Ainsi la réduction se fait naturellement. \\


\subsubsection{Preuve formelle de la réduction}
Afin de faire une preuve formelle de la sécurité EUF-CMA, nous allons changer le jeu en rajoutant une fonction de hachage. L'attaquant $\mathcal{A}$ peut maintenant faire $N_{hash}$ appelle à la fonction de hachage et ainsi obtenir des couples (m,s). Il cherchera alors à créer une signature valide pour l'un d'entre eux. On retrouve bien la question du problème DOOM. \\
De plus, la fonction de signature prends maintenant en entrée un message quelconque. Elle prend ensuite un aléa $r$ dans $\{0,1\}^{\lambda_0}$. Le tout est alors donné à la fonction de hachage qui renvoit un syndrome valide. \\

\noindent Nous allons aussi introduire un système de jeux qui nous permettra de réduire la sécurité d'un système à un problème $P$. Soit $\mathcal{A}$ un attaquant et $\mathcal{R}$ un rival. Soient $G_0, G_1, ...,G_N$ un ensemble de jeux et soit $\mathbb{P}(G_i)$ la probabilité pour $\mathcal{A}$ de répondre au défi posé par $\mathcal{R}$ pour le jeu $G_i$. $\mathbb{P}(G_0)$ est alors la probabilité de cassé le système considérer et $\mathbb{P}(G_N)$ la probabilité de répondre au problème $P$. \\
L'idée est de changer pas à pas les jeux $G_0$ à $G_N$ de façon à ce que :
$$\forall i \in {0,...,N-1}, |\mathbb{P}(G_i)-\mathbb{P}(G_i+1)| \in negl(\lambda) \Longrightarrow |\mathbb{P}(G_0)-\mathbb{P}(G_N)|  \in negl(\lambda)$$
où $\lambda$ est un paramètre de sécurité. Autrement dis, les changements sur les jeux ne changent qu'à un facteur négligeable près les probabilités de succès de l'attaquant $\mathcal{A}$. \\
Il n'est pas possible de changer le comportement de $\mathcal{A}$ puisqu'il est quelconque, en revanche nous pouvons modifier celui de $R$. \\

\begin{thm} (Réduction de sécurité).\label{DOOM} \\
Soit $N_{sign}$ le nombre de requêtes faites à l'oracle de signature. Soit $\lambda$ le paramètre de sécurité et $\lambda_0=\lambda + 2\log_2(N_{sign})$. On a :
$$Succ^{EUF-CMA}_{Wave}(t,N_{sign}) \leq 2Succ^{DOOM(n,q,k,N)}(t) +\rho(\mathcal{D}_{rand},\mathcal{D}_{pub})(t) + $$
$$ f(\mathcal{U}_{\omega},\mathcal{D}_{\omega}^{\mathbf{H}_{pk}}) + g(\epsilon) + c + \frac{N_{hash}}{2}\sqrt{\epsilon} + \frac{1}{2^{\lambda}}$$
où $\epsilon$ est une fonction en $n$ qui décroît exponentiellement, $\mathcal{D}_{rand}$ est la distribution des matrices prises aléatoirement dans $\F_q{(n-k)\times n}$, $\mathcal{D}_{pub}$ celle des matrices prises aléatoirement dans l'ensemble des matrices de parité d'un code $(U,U+V)$-généralisé (et $U$, $V$ de dimension respectives $k_U$ et $k_V$), $f$ l'espérance de la distance calculatoire entre les deux distributions, $g$ linéaire en $\epsilon$ et $c$ une constante.
\end{thm}

\begin{proof}[Preuve] (On trouvera le détail des preuves de probabilité en annexe de ce rapport.) On rappelle que $G_0$ correspond à notre jeu pour la sécurité EUF-CMA de Wave.
\begin{itemize} 
\item $G_1$ : Le jeu $G_1$ est identique au jeu $G_0$ sauf si l'évènement 
\begin{center}
$F := \{\text{Un même aléa r a été tiré lors de deux requêtes}$
$\text{\qquad d'un même message à l'oracle de signature}\}.$
\end{center}
On a alors 
$$ \mathbb{P}(G_0) \leq  \mathbb{P}(G_1) +  \mathbb{P}(F) $$
Or pour $\lambda_0=\lambda + 2\log_2(N_{sign})$, la probabilité que l'évènement $F$ se produise est majorée par $\frac{1}{2^{\lambda}}$. C'est donc négligeable et le changement est autorisé.
\item $G_2$ : Le passage au jeu $G_2$ permet d'empêcher $\A$ de faire appel à l'oracle de signature sur les syndrome du problème DOOM. L'idée est de créer une liste suffisemment grande $L_m$ d'aléas tous différents. On modifie alors la fonction \verb|hash| de cette façon :
	\begin{enumerate}
	\item Si \verb|hash| est appelée par la fonction \verb|sign|, alors les aléas seront pris successivement dans $L_m$ et associés à un vecteur erreur $\e_{m,r}$ (stocké) pris uniformément dans $S_{\omega}$. Elle renvoie alors $\s=\e_{m,r}\mathbf{H}^T$.
	\item En revanche si \verb|hash| est appelée hors de la fonction \verb|sign| par $\A$, alors elle son comportement dépendra de l'aléa. Si $r$ est dans $L_m$ elle se comporte comme si elle avait été appelée par \verb|sign| et renvoie $\e_{m,r}\mathbf{H}^T$. Sinon elle renvoie successivement les syndromes du problème DOOM.
	\end{enumerate}
On prend donc dans la fonction \verb|sign| toujours le $r$ suivant de $L_m$. On a alors changé le jeu en supprimant le cas où deux mêmes $r$ sont tirés lors de la signature. Cela ne pose pas de problème grace au passage à $G_1$. Le passage au jeu $G_2$ permettra ainsi de s'assurer par la suite que $\A$ n'a pas fait d'appel à \verb|sign| sur les syndromes du problème DOOM.
On a alors 
$$ \mathbb{P}(G_1) \leq  \mathbb{P}(G_2) +  \frac{N_{hash}}{2}\sqrt{\epsilon} $$
où $\epsilon$ est une fonction en $n$ qui décroît exponentiellement. C'est donc bien négligeable.
\item $G_3$ : Le jeu $G_3$ permet à l'oracle de signature de se passer de l'algorithme de décodage, et donc de la trappe $T$. Il sera nécessaire pour remplacer la matrice du code (U,U+V)-généralisé par la matrice aléatoire de l'instance du problème DOOM. Pour passer au jeu $G_3$, on modifie la sortie de \verb|sign|. Au lieu de renvoyer le couple $(\e,r)$ où $\e = D_{\varphi,H_U,H_V}$, on renvoit le couple $(\e_{m,r},r)$ préalablement stocké.\\
 La différence de succès de dépand que de $\omega$ et des différence de distribution entre $\mathcal{U}_{\omega}$ et $\mathcal{D}_{\omega}^{\mathbf{H}_{pk}}$, où $\mathcal{U}_{\omega}$ et la distribution uniforme sur $S_{\omega}$ et où $\mathcal{U}_{\omega}$ et $\mathcal{D}_{\omega}^{\mathbf{H}_{pk}}$ est la distribution des couples $(e,r)$ où $r$ est un aléa uniforme dans $\{0,1\}^{\lambda_0}$ et $e$ est la sortie de l'algorothme de décodage avec trappe sur une entrée $s$ prise uniformément dans $\F_q^{n-k}$.
On a alors 
$$ \mathbb{P}(G_2) \leq  \mathbb{P}(G_3) + f(\mathcal{U}_{\omega},\mathcal{D}_{\omega}^{\mathbf{H}_{pk}}) + g(\epsilon) + c$$
où $f$ et $g$ sont linéaires et $c$ un certaine constante.
\item $G_4$ : On peut maintenant remplacer $\mathbf{H}_{pk}$ par $\mathbf{H}_0$. Ce changement ne pose pas de problème puisque \verb|sign| n'utilise plus la trappe. En revanche, nous avons créé un distingueur entre la distribution $(:=\mathcal{D}_{rand})$ des matrices prises aléatoirement dans $\F_q{(n-k)\times n}$ et la distribuction $(:=\mathcal{D}_{pub})$ des matrices prises aléatoirement dans l'ensemble des matrices de parité d'un code $(U,U+V)$-généralisé où $U$ (resp. $V$) est un $[n/2,k_U]$-code (resp. $[n/2,k_V]$-code). 
On a alors 
$$ \mathbb{P}(G_3) \leq  \mathbb{P}(G_4) + \rho(\mathcal{D}_{rand},\mathcal{D}_{pub})(t)$$
\item $G_5$ : On change ici la procédure de fin. On rajoute à la vérification la condition $r \notin L_m$. Ainsi on est bien sûr que $\A$ réussi le jeu s'il répond au problème DOOM. Alors la probabilité que $\A$ réussisse $G_5$ est exactement la probabilité que $\A$ réussisse $G_4$ et $r\notin L_m$.
On a alors 
$$ \mathbb{P}(G_4) \leq  2\mathbb{P}(G_5) + \rho(\mathcal{D}_{rand},\mathcal{D}_{pub})(t)$$
où $\mathbb{P}(G_5)$ est exactement la probabilité pour $\A$ de renvoyer $\e_j \in S_{\omega}$ et tel que $\e_j\mathbf{H}_0^T = \s_j$ pour un certain indice $j$ du problème DOOM. On a donc 
$$ \mathbb{P}(G_5) \leq Succ^{n,k,N_{hash}, \omega}_{DOOM}(t)$$
\end{itemize}
En rassemblant toutes les inégalités on termine la preuve. 
\end{proof}


\subsection{Indistinguabilité des codes (U,U+V)-généralisés}

Le schéma Wave est, d'après ce qui précède, sûr à condition que l'on ne puisse pas distinguer une matrice de parité d'un code (U,U+V)-généralisé d'une matrice de parité aléatoire. \\

\noindent Nous avons, tout au long de ce rapport, simplifié les notations et les algorithmes. Afin de discuter ce problème de distinction, il est nécéssaire d'en réintroduire certaines. \\
La trappe du schéma Wave étant la structure du code $(U,U+V)$-généralisé utilisé, nous devons nous assurer qu'on ne peut pas la retrouver à partir de la clé secrète. Nous introduisons donc une matrice $S\in\F_q^{(n-k)\times(n-k)}$ inversible et une matrice de permutation $P\in\F_q^{n\times n}$, la clé publique devient alors $pk = SHP$. Les matrices $S$ et $P$ faisant donc partie de la clé privée.\\

\noindent Lorsque $\dim(U) < \dim(V)$, il est possible de montrer que ce problème est NP-complet à l'aide d'une réduction au problème du mariage tri-dimensionnel lui-même NP-complet. \\
En revanche, dans le cas où $\dim(U) > \dim(V)$ nous avons montré précédemment qu'il est facile de distinguer un code binaire $(U,U+V)$-généralisé d'un code aléatoire d'où le passage à $q>2$.
La démonstration du caractère NP-complet de Wave lorsque $q=3$ et $\dim(U) > \dim(V)$ reste non achevée pour l'instant.
Les auteurs du papier d'origine pensent néanmoins que la généralisation des codes $(U,U+V)$ comme nous l'avons présentée dans la définition \ref{UV-normalise} et notamment le grand choix des vecteurs $a$, $b$, $c$, et $d$ offrent la possibilité d'étendre la réduction au problème du mariage tri-dimensionnel.\\

\section*{Conclusion}
\addcontentsline{toc}{section}{\protect\numberline{}Conclusion}

\newpage


\begin{appendix}
\section*{Quelques preuves supplémentaires}
\addcontentsline{toc}{section}{\protect\numberline{}Quelques démonstrations supplémentaires}

\subsection*{Démonstration du lemme \ref{maj_dist_e_eunif}}

Nous allons avoir besoin dans la suite de la proposition suivante que nous ne démontrerons pas dans ce rapport :
\begin{propo}\label{rho_f}
Soient $X$ et $Y$ deux variables aléatoires dans le même espace $A$ et $Z$ une variable aléatoire dans l'espace $B$ indépendante de $X$ et $Y$. Alors, pour toute fonction $f$, nous avons : $$ \rho(f(X,Z),f(Y,Z)) \leq \rho(X,Y)$$
\end{propo}

\begin{proof}[Preuve du lemme \ref{maj_dist_e_eunif}]
Rappelons que nous voulons montrer l’inégalité :
$$ \rho\left(\e ,\e^{unif}\right) \leq \sum\limits_{x_V,l} \rho\left(J^{\mathbf{H}_U},J^{unif}\right)\mathbb{P}\left(k_0 = l, \e_V = x_V\right) $$ 

\noindent En écrivant $\e = (\e_U,\ \e_V)$ et $\e^{unif} = (\e_U^{unif},\ \e_V^{unif})$, nous pouvons majorer leur distance statistique
{\scriptsize
\begin{equation*}
\begin{split}
\rho(\e,\e^{unif}) &= \rho\Big((\e_U,\e_V),(\e_U^{unif},\e_V^{unif})\Big)\\
&\leq \sum\limits_{x_U,x_V} \left|\mathbb{P}\Big((\e_U,\e_V)=(x_U,x_V)\Big) - \mathbb{P}\Big((\e_U^{unif},\e_V^{unif})=(x_U,x_V)\Big)\right|\\
&= \sum\limits_{x_U,x_V} \left|\mathbb{P}(\e_V = x_V)\mathbb{P}(\e_U=x_U|\e_V=x_V) - \mathbb{P}(\e_V^{unif} = x_V)\mathbb{P}(\e_U^{unif}=x_U | \e_V^{unif}=x_V)\right|\\
&= \sum\limits_{x_U,x_V} \mathbb{P}(\e_V = x_V)\left|\mathbb{P}(\e_U=x_U|\e_V=x_V) - \mathbb{P}(\e_U^{unif}=x_U | \e_V^{unif}=x_V)\right|\\
\end{split}
\end{equation*}}

\noindent Le passage de la troisième ligne à la quatrième se fait car $\mathbb{P}(\e_V = x_V) = \mathbb{P}(\e_V^{unif} = x_V) $.

\noindent Rappelons que $k_0$ est le poids de $\e_V$ sur $J$, en ajoutant une condition sur la valeur de $k_0$ aux probabilités, nous obtenons donc la somme suivante :
{\scriptsize
$$ \mathbb{P}(\e_U=x_U|\e_V=x_V) - \mathbb{P}(\e_U^{unif}=x_U | \e_V^{unif}=x_V) =  $$
$$ \sum\limits_l \left[ \mathbb{P}(\e_U=x_U|k_0 = l,\e_V=x_V) - \mathbb{P}(\e_U^{unif}=x_U |k_0 = l, \e_V^{unif}=x_V) \right] \mathbb{P}\left(k_0 = l | \e_V =x_V\right)$$
}

\noindent Nous obtenons donc, comme majoration de la distance statistique, une somme sur les indices $x_U$, $x_V$ et $l$. Nous allons maintenant étudier la somme sur les indices $x_U$.\\
\noindent Les ensembles $\J$ étant inclus dans un ensemble d'information, l'aléa interne de \verb|DecodeU| et de \verb|VarDecodeU| ne dépendent pas du choix de $J^{unif}$ et $J^{\mathbf{H}_U}$. 
Nous pouvons voir que $\mathbb{P}(\e_U=x_U|k_0 = l,\e_V=x_V)$ ne dépend que de $x_U$ et $J^{\mathbf{H}_U}$ et   $\mathbb{P}(\e_U^{unif}=x_U |k_0 = l, \e_V^{unif}=x_V)$ ne dépend que de $x_U$ et $J^{unif}$. 
De plus, $J^{unif}$ et $J^{\mathbf{H}_U}$ vivent dans le même espace.
Ainsi d'après la proposition \ref{rho_f}, nous avons 
{\scriptsize
$$ \sum\limits_{x_U}\mathbb{P}(\e_U=x_U|k_0 = l,\e_V=x_V) - \mathbb{P}(\e_U^{unif}=x_U |k_0 = l, \e_V^{unif}=x_V)\quad \leq\quad \rho(J^{\mathbf{H}_U},J^{unif})$$
}

En combinant les différentes équations obtenues, nous avons alors :
\begin{equation*}
\begin{split}
\rho\left(\e ,\e^{unif}\right) &\leq \sum\limits_{x_V,l} \rho\left(J^{\mathbf{H}_U},J^{unif}\right)\mathbb{P}(\e_V = x_V)\mathbb{P}\left(k_0 = l| \e_V = x_V\right)\\
&=\sum\limits_{x_V,l} \rho\left(J^{\mathbf{H}_U},J^{unif}\right)\mathbb{P}\left(k_0 = l, \e_V = x_V\right)
\end{split}
\end{equation*}
\end{proof}

\subsection*{Démonstration du lemme \ref{esp}}

\begin{lemme}\label{proba_rang}
Soient $d$ et $m$ deux entiers tels que $d<m$ et $M \hookleftarrow\F_3^{(m-d)\times m}$ alors $\mathbb{P}(rg(M) < m-d) \leq \frac{1}{2 . 3^d}$
\end{lemme}

\begin{proof}[Preuve]
Soit $P$ la probabilité que $M$ ne soit pas de rang plein, c'est-à-dire de rang inférieur à $m-d$.
Notons $V_i$ le sous-espace vectoriel engendré par les $i$ premières lignes de $M$. Si $M$ n'est pas de rang plein c'est qu'au moins une de ses lignes est une combinaison linéaire des précédentes, c'est-à-dire qu'il existe un $i$ tel que $\dim(V_i) = \dim(V_{i-1}) = i-1$. On a alors
\begin{equation*}
\begin{split}
P &\leq \sum\limits_{i=1}^{m-d} \mathbb{P}(\dim(V_i) = \dim(V_{i-1}) = i-1) \\
&= \sum\limits_{i=1}^{m-d} \mathbb{P}(\dim(V_i) =i-1 | \dim(V_{i-1}) = i-1)\mathbb{P}(\dim(V_{i-1}) = i-1) \\
&\leq \sum\limits_{i=1}^{m-d} \mathbb{P}(\dim(V_i) =i-1 | \dim(V_{i-1}) = i-1) \\
\end{split}
\end{equation*}
Si $\dim(V_{i-1}) = i-1$, alors $\dim(V_i) = i-1$ est équivalent à dire que la ligne $i$ de la matrice est une combinaison linéaire des $i-1$ précédentes. Nous avons donc :
$$ \mathbb{P}(\dim(V_i) =i-1 | \dim(V_{i-1}) = i-1) = \frac{3^{i-1}}{3^m} = \frac{1}{3^{m-i+1}} $$
Nous pouvons donc conclure avec :
\begin{equation*}
\begin{split}
P &\leq \sum\limits_{i=1}^{m-d} \frac{1}{3^{m-i+1}}\\
& = \frac{1}{2.3^{-d}} - \frac{1}{2;3^m}\\
&\leq  \frac{1}{2.3^{-d}} 
\end{split}
\end{equation*}
\end{proof}

\begin{proof}[Preuve du lemme \ref{esp}]

Nous voulons ici montrer que $$ \mathbb{E}\left(\rho\left(J_{x_V,l}^{unif},J_{x_V, l}^{\mathbf{H}_U}\right)\right) \leq \frac{3^{-d}}{2} $$
Notons $n_{U,d}$ le nombre de sous-ensembles de $\llbracket 1, n/2\rrbracket$ de taille $k_U-d$ ie \mbox{$n_{U,d} = $ {\scriptsize$\dbinom{n/2}{k_U-d}$}}. On a alors $$\rho(J^{unif},\ J^{\mathbf{H}_U}) = \frac{N}{n_{U,d}}$$ avec $N = \#\{J \subset \llbracket 1,\ n/2\rrbracket \text{ de taille } k_U - d \text{ mauvais pour } \mathbf{H}_U\}$.\\
Notons $X_i$ l'événement "le sous-ensemble $\varepsilon_i$ est un mauvais ensemble pour $\mathbf{H}_U$", c'est-à-dire la matrice $M_{\overline{\varepsilon_i}}$ n'est pas de rang plein. D'après le lemme \ref{proba_rang}, nous avons $\mathbb{P}(X_i = 1)\leq \frac{1}{2.3^d}$
De plus, nous pouvons maintenant écrire $N = \sum\limits_{i=1}^{n_{U,d}}X_i$.\\
Ainsi nous avons :
\begin{equation*}
\begin{split}
\mathbb{E}\left(\rho\left(J_{x_V,l}^{unif},J_{x_V, l}^{\mathbf{H}_U}\right)\right)  &= \mathbb{E}\left(\frac{N}{n_{U,d}}\right)  \\
&= \mathbb{E}\Bigg(\frac{\sum\limits_{i=1}^{n_{U,d}}X_i}{n_{U,d}}\Bigg)\\
&= \sum\limits_{i=1}^{n_{U,d}}\mathbb{E}\left(\frac{X_i}{n_{U,d}}\right)\\
&= \sum\limits_{i=1}^{n_{U,d}}\frac{\mathbb{P}(X_i = 1)}{n_{U,d}}\\
&\leq \frac{1}{2.3^d}\\
\end{split}
\end{equation*}
\end{proof}

\subsection*{Démonstration de la propostion \ref{dim_hull_UV}}
\begin{proof}[Preuve de la proposition \ref{dim_hull_UV}]
La dimension d'un $hull$ étant invariante par permutation nous allons ici montrer que nous avons $dim(hull(U,U+V)) = k_U - k_V$ avec probabilité $1-\mathcal{O}(2^{k_U-k_V})$.
Par définition du $hull$, nous avons :
\begin{equation*}
\begin{split}
hull(U,U+V) &= (U,U+V) \cap (U,U+V)^{\bot } \\
&= (U,U+V) \cap (V^{\bot}+U^{\bot}, V^{\bot}) \\
\end{split}
\end{equation*} 
Ainsi $\forall u\in U$ et $\forall v\in V$ tels que $(u,u+v)\in hull(U,U+V)$, il existe $u_t\in U^{\bot}$ et $v_t\in V^{\bot}$ tels que

\begin{equation*}
\left\{
\begin{aligned}
&u = v_t + u_t\\
&u + v = v_t\\
\end{aligned}
\right.
\iff
\left\{
\begin{aligned}
&v = u_t\\
&u + v = v_t\\
\end{aligned}
\right.
\end{equation*}
Donc $v\in V\cap U^{\bot}$.
De plus, nous avons $$\dim(V) + \dim(U^{\bot}) = \frac{n}{2} +k_V - k_U < \frac{n}{2}$$
Alors, avec probabilité $1-0(2^{k_V-k_U})$ sur le choix des codes, nous aurons $V \cap U^{\bot} = {0}$ et $v=u^{\bot} = 0$. Ainsi, les vecteurs de hull$(U,U+V)$ seront de la forme $(x,x)$ où $x \in U \cap V^{\bot}$ avec probabilité  $1-0(2^{k_V-k_U})$ .
De même on a,
$$\dim(U) + \dim(V^{\bot}) = \frac{n}{2} +k_U - k_V < \frac{n}{2}$$

Alors avec probabilité $1-0(2^{k_V-k_U})$, on aura 
$$\dim(\text{hull}((U,U+V))) = \dim(U\cap V^{\bot}) = k_U-k_V $$
ce qui conclue la preuve.
\end{proof}

\subsection*{Démonstration du théorème \ref{DOOM}}
\begin{proof}[Preuve du théorème \ref{DOOM}] \
\begin{itemize}
\item \textbf{Passage de $G_0$ à $G_1$ : } Montrons que $$\mathbb{P}(F) \leq \frac{1}{2^\lambda}$$.
Remarquons d'abord que la probabilité de n'avoir aucune collision sur $t$ tirages indépendants et uniforme dans un ensemble de taille $n$ est majoré par $t^2/n$ (admis). Dans notre cas, $t=N_{sign}$ et $n=2^{\lambda_0}$. Ainsi avec $\lambda_0 = \lambda +2 \log_2N_{sign}$, on en déduit que 
$$ \mathbb{P}(F) \leq \frac{N^2_{sign}}{2^{\lambda_0}}=\frac{1}{2^{\lambda_0 -2 \log_2N_{sign}}} =\frac{1}{2^\lambda}$$ ce qui conclu la preuve.
\item \textbf{Passage de $G_1$ à $G_2$ : } Montrons que $$ \mathbb{P}(G_1) \leq  \mathbb{P}(G_2) +  \frac{N_{hash}}{2}\sqrt{\epsilon} $$
où $\epsilon$ est une fonction en $n$ qui décroît exponentiellement.
Nous utiliserons la proposition suivante (admise) :
\begin{propo} Soient les variables aléatoires discrètes $X_i$ et $Y_i$ où $i \in \{1,2\}$ de même domaine $\mathcal{A}_i$. Notons pour $a_i \in \mathcal{A}_i$ :
	\begin{itemize}
	\item $p(.|a_i)$ la distribution de $X_2$ conditionnée à l'évènement $X_1 = a_i$
	\item $q(.|a_i)$ la distribution de $Y_2$ conditionnée à l'évènement $Y_1 = a_i$
	\end{itemize}
Alors nous avons,
$$\rho(X_1,X_2,Y_1,Y_2) \leq \sup_{a_i \in \mathcal{A}_i}\rho(p(.|a_i),q(.|a_i)) + \rho(X_1,Y_1).$$
\end{propo}
Les distributions des jeux $G_1$ et $G_2$ diffèrent par les sorties de la fonction de hachage. Dans le jeu $G_1$ les sorties $X_i$ sont uniformément distribuées dans $\F_q^{n-k}$. Dans le jeu  $G_2$, les sorties $Y_i$ valent $\e\mathbf{H}^T$ avec $e$ uniformément distribué dans $S_{\omega}$ si $r\in L_m$, sinon $Y_i$ est uniformément distribuée.  On a,
\begin{equation}
\begin{split}
\mathbb{P}(G_1) - \mathbb{P}(G_2) &= \sum_{\mathbf{H}}\mathbb{P}(\mathbf{H}_{pk}=\mathbf{H})(\mathbb{P}(G_1|\mathbf{H}_{pk}=\mathbf{H})-\mathbb{P}(G_1|\mathbf{H}_{pk}=\mathbf{H}) \\
&\leq \mathbb{E}_{\mathbf{H}_{pk}}(\rho(X_1,...,X_{N_{hash}},Y_1,...,Y_{N_{hash}}))\\
\end{split}
\end{equation}
\begin{propo} Soit $X$ la distribution uniforme dans $\F_q^{n-k}$ et $Y$ la distribution de $\e\mathbf{H}^T$ avec $e$ uniformément distribué dans $S_{\omega}$. Alors
$$\mathbb{E}_{H_{pk}}(\rho(X,Y)) \leq \frac{1}{2}\sqrt{\epsilon} $$
où $\epsilon$ est une borne qui décroit exponentiellement avec $n$.
\end{propo}
Par les deux propositions précédentes et par un raisonnement par récurrence, on termine la preuve. 
\item \textbf{Passage de $G_2$ à $G_3$ : } Nous avons
$$ \mathbb{P}(G_2) \leq  \mathbb{P}(G_3) + f(\mathcal{U}_{\omega},\mathcal{D}_{\omega}^{\mathbf{H}_{pk}}) + g(\epsilon) + c$$
où $f$ et $g$ sont linéaires et $c$ un certaine constante.
La fonction en $\epsilon$ découle d'un raisonnement similaire au raisonnement précédent. Ensuite, à constante près, les différences de distributions entre les jeux $G_2$ et $G_3$ ne dépendent que de l'espérance de la distance statistique entre les distributions $\mathcal{U}_{\omega}$ et $\mathcal{D}_{\omega}^{\mathbf{H}_{pk}}$, où $\mathcal{U}_{\omega}$ et la distribution uniforme sur $S_{\omega}$ et où $\mathcal{U}_{\omega}$ et $\mathcal{D}_{\omega}^{\mathbf{H}_{pk}}$ est la distribution des couples $(e,r)$ où $r$ est un aléa uniforme dans $\{0,1\}^{\lambda_0}$ et $e$ est la sortie de l'algorothme de décodage avec trappe sur une entrée $s$ prise uniformément dans $\F_q^{n-k}$.
\item \textbf{Passage de $G_3$ à $G_4$ : } La différence entre ces deux jeux ne vient que de la distance entre la distributions des matrices de parité de nos codes $(U,U+V)$-généralisés et celle des matrices prises uniformément dans $\F_q^{n-k}$.
\item \textbf{Passage de $G_4$ à $G_5$ : } Le jeu $G_5$ diffère du jeu $G_4$ uniquement par la vérification finale. Soit le couple $(e,r)$ la réponse de $\mathcal{A}$ comme signature d'un message $m$. Sa probabilité de succès pour le jeu $G_5$ est exatement la probabilité de l'évènement "réussit $G_4$ et $r\notin L_m$".  \\
Si la signature est valise, alors $m$ n'a jamais été demandé à l'oracle de signature, donc $\mathcal{A}$ n'a jamais eu accès aux éléments de $L_m$. Donc les évenements sont indépendants et nous avons,
$$\mathbb{P}(G_5) = (1 - 2^{- \lambda_0})^{N_{sign}}\mathbb{P}(G_4).$$
où $\lambda_0 \leq \log_2(N²_{sign})$. Donc
$$(1-2^{-\lambda_0})^{N_{sign}} \geq (1-\frac{1}{N²_{sign}})^{N_{sign}} \geq \frac{1}{2} $$

D'où $\mathbb{P}(G_5) \geq\frac{1}{2}\mathbb{P}(G_4)$.
\end{itemize}

\end{proof}


\end{appendix}

\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}
